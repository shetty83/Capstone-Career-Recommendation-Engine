{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pulling in Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/coreyyoung/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from scipy.cluster.hierarchy import dendrogram, ward\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 50, 20\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "nltk.download('stopwords')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pulling in the data, cleaning it, and organizing it for use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pull in data\n",
    "\n",
    "work_ex = pd.read_csv('car_muse.csv').dropna()\n",
    "skills = pd.read_csv('skills.csv').dropna()\n",
    "\n",
    "# Make dataframe of work_ex with only the first instance of job title\n",
    "grpdby_title = work_ex[['resume_id', 'title', 'description']].groupby('resume_id').first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add space in front of each skill to prepare for group-by\n",
    "\n",
    "listy = []\n",
    "for i in skills['value']:\n",
    "    listy.append(' ' + i)\n",
    "skills['value'] = listy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Group skills table by resume_id\n",
    "\n",
    "grpd_skills = skills[['resume_id', 'value']].groupby('resume_id').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Merge the two tables into new dataframe, fill na with empty string to bypass errors downstream\n",
    "\n",
    "merged = grpdby_title.merge(grpd_skills, how='outer', left_index=True, right_index=True).fillna('')\n",
    "merged.columns = ['title', 'description', 'skills']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:55: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n"
     ]
    }
   ],
   "source": [
    "# Functions from Jesse to clean skills data and make a vocabulary for skills vectorization\n",
    "\n",
    "def text_scrubber(values):\n",
    "    \n",
    "    '''\n",
    "        Replace problematic phrases and unicode characters.\n",
    "        \n",
    "        (10+ years) ---> ''\n",
    "        &#39; , \\x92 ---> apostrophe\n",
    "        Technical: ---> ''\n",
    "        \n",
    "        These characters act usually as separators, so replace them with commas for splitting later.\n",
    "            [:, ;, &amp;, \\x95, &, ., /, 'and'] ---> ','\n",
    "        \n",
    "    '''\n",
    "\n",
    "    result = []\n",
    "    for string in values:\n",
    "        # Regex as explained above\n",
    "        temp = re.sub('(\\(.*\\))', '', string)\n",
    "        temp = re.sub('&#39;|\\x92', '\\'', temp)\n",
    "        temp = re.sub(' &amp; |&amp;|\\x95|:|;|&|\\.|/| and ', ',', temp)\n",
    "        temp = re.sub('\\w*:\\s+', ', ', temp)\n",
    "        \n",
    "        result.append(temp)\n",
    "        \n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "def tokenizer(df):\n",
    "    \n",
    "    \n",
    "    '''\n",
    "        Parse the given skills dataframe to pull out appropriate skill phrases.\n",
    "        Dataframe has some cells that are 2-gram nicely made skills, and other cells\n",
    "        that are long runons with many skills.\n",
    "        After scrubbing and then splitting on commas, we simplify the task by tossing\n",
    "        out any greater than 4-gram phrases. \n",
    "    '''\n",
    "    \n",
    "    # Custom stop words that come up very often but don't say much about the job title.\n",
    "    stops = ['manager', 'responsibilities', 'used', 'skills', 'duties', 'work', 'worked', 'daily',\n",
    "             'services', 'job', 'using', '.com', 'end', 'prepare', 'prepared', 'lead', 'requirements','#39'] + list(stopwords.words('english'))\n",
    "\n",
    "    values, ids, resume_ids = [],[],[]\n",
    "    count = 0\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        \n",
    "        # Split on commas\n",
    "        array = row['value'].lower().split(',')\n",
    "        for x in array:\n",
    "            # make sure the value is not empty or all numeric values or in the stop words list\n",
    "            if x != '' and not x in stops and not x.lstrip().rstrip().isdigit():\n",
    "                # make sure single character results are the letter 'C' (programming language)\n",
    "                if len(x) > 1 or x == 'C':\n",
    "                    # drop stuff > 4 gram\n",
    "                    if len(x.split(' ')) <= 4:\n",
    "                        # update lists\n",
    "                        \n",
    "                        values.append(x.lstrip().rstrip())\n",
    "                        ids.append(count)\n",
    "                        count+=1\n",
    "                        resume_ids.append(row['resume_id'])\n",
    "    \n",
    "    # New dataframe with updated values.\n",
    "    result_df = pd.DataFrame()\n",
    "    result_df['id'] = ids\n",
    "    result_df['resume_id'] = resume_ids\n",
    "    result_df['value'] = values\n",
    "    return result_df\n",
    "\n",
    "df = skills.copy()\n",
    "\n",
    "df['value'] = text_scrubber(df['value'])\n",
    "test_df = tokenizer(df)\n",
    "\n",
    "voc = test_df['value'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create dataframe for each job title by repeating above steps\n",
    "\n",
    "merged = grpdby_title.merge(grpd_skills, how='outer', left_index=True, right_index=True)\n",
    "merged.columns = ['title', 'description', 'skills']\n",
    "merged = merged[pd.notnull(merged['title'])]\n",
    "merged.fillna('', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top_titles = merged['title'].str.lower().str.strip().value_counts().index[:2910]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for_jt_mat = merged[merged['title'].str.strip().str.lower().isin(top_titles)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "for_jt_mat['title'] = for_jt_mat['title'].str.strip().str.lower()\n",
    "for_jt_mat['title'].value_counts()\n",
    "sample2 = for_jt_mat.groupby('title').sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorizing job descriptions and skills for features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Vectorize text of job descriptions, put resulting matrix into dataframe\n",
    "sample2['description'] = [x.replace(\"&nbsp;\",\" \").replace(\"\\x92\",\" \").replace(\"\\x95\",\" \").replace('&amp;',\" \") \\\n",
    "                             .replace('*',\" \").replace(\".\",\" \").replace(\"co&#39;s\",\"\").replace(\"\\xae&quot;\",\"\") \\\n",
    "                             .replace(\"&#39;s\",\"\").replace(\"&quot;\",\"\").replace(\"?\",\"\").replace(\"&#39;s\",\"\") \\\n",
    "                             .replace(\"@\",\"\").replace(\"\\x96\",\"\")\n",
    "                             for x in sample2['description']]\n",
    "\n",
    "mine = ['manager', 'amp', 'nbsp', 'responsibilities', 'used', 'skills', 'duties', 'work', 'worked', 'daily',\n",
    "       'services', 'job', 'using', 'com', 'end', 'prepare', 'prepared', 'lead', 'requirements']\n",
    "vec = TfidfVectorizer(analyzer='word', ngram_range=(1,2), token_pattern='[a-zA-z]{3,50}', max_df=0.2, min_df=2,\n",
    "                      max_features=10000, stop_words=text.ENGLISH_STOP_WORDS.union(mine), decode_error='ignore', vocabulary=None, binary=False)\n",
    "\n",
    "description_matrix2 = vec.fit_transform(sample2['skills']+sample2['description'])\n",
    "description_matrix2 = pd.DataFrame(description_matrix2.todense())\n",
    "description_matrix2.columns = vec.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Vectorize skills text with scrubby as vocab, put resulting matrix into dataframe\n",
    "vec2 = TfidfVectorizer(vocabulary=voc, decode_error='ignore')\n",
    "skills_matrix2 = vec2.fit_transform(sample2['skills']+sample2['description'])\n",
    "skills_matrix2 = pd.DataFrame(skills_matrix2.todense())\n",
    "skills_matrix2.columns = vec2.get_feature_names()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Combine skills and description matrices into combined job title feature matrix\n",
    "\n",
    "jobtitle_matrix = pd.concat([skills_matrix2, description_matrix2], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run PCA to reduce number of features for clustering, logistic regression, and cosine similarity calculations\n",
    "Ideally, this would run on a scheduled basis as more data came in to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python2.7/site-packages/pandas/indexes/base.py:1237: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n",
      "  return key in self._engine\n"
     ]
    }
   ],
   "source": [
    "# Run PCA to reduce number of features\n",
    "\n",
    "pca = PCA(n_components=1000, random_state=42)\n",
    "comps = pca.fit_transform(jobtitle_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Put the components into a dataframe\n",
    "\n",
    "comps = pd.DataFrame(comps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster data into similar titles to increase efficiency of cosine similarity calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cluster job titles based on components derived from feature matrix\n",
    "\n",
    "cltr = AgglomerativeClustering(n_clusters=8)\n",
    "cltr.fit(comps)\n",
    "\n",
    "# Add new column containing cluster number to sample, comps, and feature matrix dataframes\n",
    "\n",
    "sample2['cluster_no'] = cltr.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train multinomial logistic regression to assign clusters to incoming users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Assign X, y, and the train test split\n",
    "X = comps\n",
    "y = sample2['cluster_no']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=10, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=1000, multi_class='multinomial',\n",
       "          n_jobs=1, penalty='l2', random_state=None, solver='sag',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform Logistic regression.  At this point, really to judge features.\n",
    "lr = LogisticRegression(C=10, penalty='l2', multi_class='multinomial', solver='sag', max_iter=1000)\n",
    "lr.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Assign cluster number to each job title in comps to pull particular cluster out for comparison\n",
    "comps['cluster_no'] = y.values\n",
    "comps.set_index('cluster_no', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Suggest jobs based on user resume text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLUSTER NUMBER 0 \n",
      "\n",
      "\n",
      "Top ten suggested for your cluster \n",
      "                                                       score\n",
      "title                                                       \n",
      "senior product marketing manager - advertising/...  0.289186\n",
      "assistant professor                                 0.246932\n",
      "business data analyst                               0.213834\n",
      "senior software engineering staff                   0.200695\n",
      "research                                            0.184371\n",
      "graduate research assistant                         0.175756\n",
      "senior product manager, retail leadership devel...  0.169874\n",
      "teacher&#39;s assistant                             0.166668\n",
      "senior analyst, strategic planning &amp; busine...  0.163795\n",
      "after school counselor                              0.158036 \n",
      "\n",
      "\n",
      "Top five suggested in cluster 0 \n",
      "                                                       score\n",
      "title                                                       \n",
      "senior product marketing manager - advertising/...  0.289186\n",
      "assistant professor                                 0.246932\n",
      "business data analyst                               0.213834\n",
      "senior software engineering staff                   0.200695\n",
      "research                                            0.184371 \n",
      "\n",
      "\n",
      "Top five suggested in cluster 1 \n",
      "                                       score\n",
      "title                                       \n",
      "data analyst                        0.284798\n",
      "data scientist                      0.249050\n",
      "sr. data analyst                    0.247436\n",
      "business analyst at harland-clarke  0.245699\n",
      "quantitative research analyst       0.244824 \n",
      "\n",
      "\n",
      "Top five suggested in cluster 2 \n",
      "                                                       score\n",
      "title                                                       \n",
      "assistant vice president of digital &amp; brand...  0.282339\n",
      "digital marketing manager                           0.241835\n",
      "manager, product marketing &amp; analytics          0.224240\n",
      "paid search strategist                              0.200816\n",
      "digital marketing specialist                        0.191945 \n",
      "\n",
      "\n",
      "Top five suggested in cluster 3 \n",
      "                                                       score\n",
      "title                                                       \n",
      "independent insurance agent                         0.029472\n",
      "regional director                                   0.024578\n",
      "assistant director of human resources               0.012211\n",
      "senior hr business partner - global hr programs...  0.009990\n",
      "erm - hr mgr                                        0.007681 \n",
      "\n",
      "\n",
      "Top five suggested in cluster 4 \n",
      "                                        score\n",
      "title                                        \n",
      "finance consultant                   0.142090\n",
      "accounting manager/ project manager  0.087983\n",
      "financial analytics consultant       0.075071\n",
      "business analytics acting manager    0.069819\n",
      "financial analyst                    0.060796 \n",
      "\n",
      "\n",
      "Top five suggested in cluster 5 \n",
      "                                 score\n",
      "title                                 \n",
      "sales floor team member       0.098340\n",
      "senior merchandising manager  0.074715\n",
      "operations associate          0.069067\n",
      "retail store manager / amt    0.056285\n",
      "managing director - usa       0.055558 \n",
      "\n",
      "\n",
      "Top five suggested in cluster 6 \n",
      "                                         score\n",
      "title                                         \n",
      "assistant principal                   0.238514\n",
      "instructor                            0.222244\n",
      "special education instructional aide  0.214482\n",
      "long-term substitute teacher          0.209600\n",
      "senior academic counselor             0.200613 \n",
      "\n",
      "\n",
      "Top five suggested in cluster 7 \n",
      "                           score\n",
      "title                           \n",
      "engineering recruiter   0.043674\n",
      "corporate it recruiter  0.042154\n",
      "sr technical recruiter  0.031964\n",
      "recruiter coordinator   0.027276\n",
      "technical sourcer       0.022163 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Putting resume as text into the system for suggestions\n",
    "resume_text = '''\n",
    "Matt Brown\n",
    "Student - Data Science Immersive at General Assembly\n",
    "m.brown3255@gmail.com\n",
    "Summary\n",
    "I am currently taking a 3 month leave of absence from Amazon to take the full-time Data Science Immersive course at General Assembly (full details in experience below) while continuing to learn online (Treehouse + Udemy + Codecademy) with the goal to come back working as a data professional.\n",
    "I am very passionate about using data to increase productivity/automation and create better customer experiences. Before this course and working at Amazon, I worked at Target.com, Sears.com, and a web analytics/big data start up, Dataium (acquired by IHS; NYSE). I went to school at Indiana University where I majored in Marketing/Operations, was a Teacher Assistant for a business course called Technology in Business, and started a cake delivery company (MyCampusCakes.com, more details below) where we delivered cakes to students on their birthdays (typically ordered by their parents).\n",
    "Experience\n",
    "Student - Data Science Immersive at General Assembly\n",
    "October 2016 - Present (3 months)\n",
    "General Assembly - Data Science Immersive is a 3-month full-time in-person course that teaches the skills\n",
    "required to become a Data Scientist including Python, SQL, Mongo-DB, Machine Learning Modeling (classification, regression, and clustering) , Tableau, etc.\n",
    "Full information regarding this course can be found at the below link:\n",
    "https://generalassemb.ly/education/data-science-immersive\n",
    "Brand Specialist - Video Games, Shoes at Amazon\n",
    "February 2014 - October 2016 (2 years 9 months)\n",
    "Brand Specialist in Video Games on the Microsoft account (Xbox, Halo/Gears of War/Minecraft, Digital,\n",
    "etc.). Responsible for Marketing/Site Experiences, Inventory Management/Supply Chain, Digital selection/ CX growth, and project roll outs. Was previously on the Rockport shoes account until June 2015.\n",
    "Senior Business Analyst - Target.com at Target\n",
    "June 2012 - February 2014 (1 year 9 months)\n",
    "• Managed $50M+ worth of inventory flow and instocks for Entertainment (Movies, Music, and Books) and\n",
    "  Girls Toys for Target.com.\n",
    "Page1\n",
    "• Invented new process and analytics suite to predict store new release sales using online real-time web analytics metrics resulting in increased buys on key new release titles and the SVP and VP Multichannel Merchandising Awards.\n",
    "• Created and trained Target.com merchandising team on web analytics suite Adobe Site Catalyst (Omniture) • Provided Ad Hoc reporting for Buyers and extended team for sales, inventory, and web analytics metrics.\n",
    "Owner, Co-Founder at MyCampusCakes.Com\n",
    "February 2011 - June 2012 (1 year 5 months)\n",
    "Co-founded MyCampusCakes.Com with my college roommate. MyCampusCakes.com was a website/ service that allowed parents of students at Indiana University to send their student a birthday cake/cupcakes,\n",
    "balloons/utensils, and a card/note to their door! Check out the article below written about our launch on the front page of our student newspaper, the IDS!\n",
    "IHSAA Basketball Referee at IHSAA\n",
    "December 2010 - June 2012 (1 year 7 months)\n",
    "I am a licensed IHSAA basketball referee and I ref anywhere from Kindergartners to High School to Mens'\n",
    "League games.\n",
    "X201 Technology in Business: Teaching Assistant at Kelley School of Business\n",
    "December 2010 - May 2012 (1 year 6 months)\n",
    "• Selected by faculty and other teaching assistants to guide 30+ students on in-class Excel projects and\n",
    "coursework.\n",
    "• Led five project review sessions, two practical review sessions, and proctored final exams • Held weekly office hours to help 20+ students on course material\n",
    "• Created and updated supplementary Excel review files for student’s exam preparation\n",
    "Buyer Apprentice Intern - Online Consumer Electronics at Sears Holdings Corporation\n",
    "May 2011 - August 2011 (4 months)\n",
    "• Developed 2 dynamic user-friendly Excel Dashboard tools that automatically determine online TV and\n",
    "Tablet assortment gaps of over 2,100 SKU’s.\n",
    "• Created a project path diagram and financial forecasting spreadsheet of over 55,000 Marketplace SKU’s and\n",
    "$106,000 in incremental revenue.\n",
    "Summer Analyst at Dataium\n",
    "May 2010 - August 2010 (4 months)\n",
    "• Learned the ins and outs of web analytics including cookie-gathering, click maps, and click analytics (visits,\n",
    "LTV ratio, page views, click thru rate, conversion, bounce rate, exit/entry pages, referral traffic, conversional\n",
    "funnels, etc) of over 4 million unique users.\n",
    "• Worked with website developers to analyze click-maps of over 100 domains. • Utilized Salesforce.Com to find, shift, and coordinate over 3,000 leads.\n",
    "• Completed web technology audits for several hundred websites.\n",
    "Page2\n",
    "Mens Basketball League Coordinator at Lifetime Fitness\n",
    "December 2006 - July 2009 (2 years 8 months)\n",
    "Helped determine scheduling, pricing, stats, and processing for the Mens Basketball League at Lifetime\n",
    "Fitness.\n",
    "Honors and Awards\n",
    "SVP and VP Multichannel Merchandising Award\n",
    "Target Corporation September 2013\n",
    "Utilized site analytics and pre-orders to predict store sales resulting in increased buys on key entertainment titles that ended up over-performing in stores during its street week sales.\n",
    "Divisional Thought Leadership Award\n",
    "Target Corporation February 2013\n",
    "Enhanced Target.com Exclusive Entertainment SKU process resulting in increased pre-orders via assortment parity with the stores, longer pre-order windows, increased instocks, internal search optimization, and increased PDP content.\n",
    "Skills & Expertise\n",
    "Analysis\n",
    "SQL\n",
    "Python\n",
    "Microsoft Excel Tableau\n",
    "MongoDB E-commerce Online Advertising PowerPoint Leadership\n",
    "Data Analysis\n",
    "Strategic Planning Sales\n",
    "Market Research Inventory Management Microsoft Office Project Management Digital Strategy\n",
    "Public Speaking Event Planning Analytics Merchandising\n",
    "  Page3\n",
    "VBA\n",
    "Social Media Marketing Retail\n",
    "Management\n",
    "Process Improvement Microsoft Word\n",
    "Social Media\n",
    "Time Management\n",
    "Web Analytics Functions\n",
    "Financial Analysis Social Networking Critical Thinking Spreadsheets Accounting\n",
    "User Interface\n",
    "Account Management Statistics\n",
    "Business\n",
    "Pricing\n",
    "Marketing Analytics HTML\n",
    "Macro\n",
    "Pivot Tables\n",
    "Teamwork\n",
    "Financial Modeling\n",
    "Education\n",
    "General Assembly\n",
    "Data Science Immersive, 2016 - 2016\n",
    "Indiana University Bloomington\n",
    "Bachelor of Science in Business, Marketing and Operations - Kelley School of Business, 2008 - 2012\n",
    "Interests\n",
    "Internet of Things, Voice AI (Alexa/Echo, Google Home, etc), Action-Leading Business Intelligence, E- commerce, Basketball, and Video Games.\n",
    "Projects\n",
    "Halo 5 Tableau Dashboard\n",
    "October 2016 to Present Members:Matt Brown\n",
    "   Page4\n",
    "This is a Tableau Dashboard I created for my personal Halo 5 (video game) stats. I've always wanted to be able to easily filter by a time-frame and see the most important statistics including Win-Loss Record, Kill/\n",
    "Death Ratio, Top Games, Kill/Death Ratio over time, etc.\n",
    "I created this by plugging into the Halo 5 Public API (https://developer.haloapi.com), data munging with python, and feeding into Tableau.\n",
    "Courses\n",
    "Independent Coursework\n",
    "Python Basics - TeamTreeHouse.com\n",
    "SQL Basics & Modifying Data with SQL - TeamTreeHouse.com\n",
    "Python - CodeCademy.com\n",
    "Learn SQL - CodeCademy.com\n",
    "Learn GIT - CodeCademy.comf\n",
    "Certifications\n",
    "  Try SQL\n",
    "Code School\n",
    "Python\n",
    "Codecademy\n",
    "Learn SQL\n",
    "Codecademy\n",
    "Learn GIT\n",
    "Codecademy\n",
    "SQL Basics\n",
    "Treehouse Inc.\n",
    "November 2016\n",
    "November 2016\n",
    "October 2016\n",
    "October 2016\n",
    "October 2016\n",
    "Modifying Data with SQL\n",
    "Treehouse Inc. October 2016\n",
    "Reporting with SQL\n",
    "Treehouse Inc. October 2016\n",
    " Page5\n",
    "Matt Brown\n",
    "Student - Data Science Immersive at General Assembly\n",
    "m.brown3255@gmail.com\n",
    "Contact Matt on LinkedIn\n",
    "          Page6\n",
    "\n",
    "'''\n",
    "\n",
    "# Vectorize user's skills and job descriptions\n",
    "desc = pd.DataFrame(vec.transform([resume_text]).todense())\n",
    "skillz = pd.DataFrame(vec2.transform([resume_text]).todense())\n",
    "mat = pd.concat([skillz, desc], axis=1)\n",
    "\n",
    "# Tranform feature matrix with pca\n",
    "user_comps = pd.DataFrame(pca.transform(mat))\n",
    "\n",
    "# Predict cluster for user and print cluster number\n",
    "cluster = lr.predict(user_comps)[0]\n",
    "print 'CLUSTER NUMBER', cluster, '\\n\\n'\n",
    "\n",
    "# Calculate cosine similarity\n",
    "cos_sim = pd.DataFrame(cosine_similarity(user_comps,comps[comps.index==cluster]))\n",
    "\n",
    "# Get job titles from sample2 to associate cosine similarity scores with jobs\n",
    "samp_for_cluster = sample2[sample2['cluster_no']==cluster]\n",
    "cos_sim = cos_sim.T.set_index(samp_for_cluster.index)\n",
    "cos_sim.columns = ['score']\n",
    "\n",
    "# Print the top ten suggested jobs for the user's cluster\n",
    "print 'Top ten suggested for your cluster', '\\n', cos_sim.sort_values('score', ascending=False)[:10], '\\n\\n'\n",
    "\n",
    "# Print the top five suggested jobs for each cluster\n",
    "for i in range(8):\n",
    "    cos_sim = pd.DataFrame(cosine_similarity(user_comps,comps[comps.index==i]))\n",
    "    samp_for_cluster = sample2[sample2['cluster_no']==i]\n",
    "    cos_sim = cos_sim.T.set_index(samp_for_cluster.index)\n",
    "    cos_sim.columns = ['score']\n",
    "\n",
    "    print 'Top five suggested in cluster', i,  '\\n', cos_sim.sort_values('score', ascending=False)[:5], '\\n\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
